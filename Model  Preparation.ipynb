{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4477f8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "import gradio as gr\n",
    "\n",
    "from fastapi import FastAPI, UploadFile, File\n",
    "import uvicorn\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774a7023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "MODEL_NAME = \"gemma3:4b\" \n",
    "\n",
    "def ask_llm(messages):\n",
    "    \"\"\"\n",
    "    Convert messages to prompt and ask Ollama model locally.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prompt = \"\\n\".join(f\"{msg['role']}: {msg['content']}\" for msg in messages)\n",
    "\n",
    "        response = ollama.chat(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        return response['message']['content']\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"LLM Error: {str(e)}\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
